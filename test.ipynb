{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset json (C:/Users/KevinChou/.cache/huggingface/datasets/json/default-6a36e5922eb05f45/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b2892db6ec5346ca82594f1c3c41ce24",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset json (C:/Users/KevinChou/.cache/huggingface/datasets/json/default-891c751e5dc163e9/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "05057664697341298f89f4f9b3a63138",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset json (C:/Users/KevinChou/.cache/huggingface/datasets/json/default-0cae1a5ed046e7ee/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae2fa1255b194cb0b5bd087201f8b6dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at C:\\Users\\KevinChou\\.cache\\huggingface\\datasets\\json\\default-6a36e5922eb05f45\\0.0.0\\e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4\\cache-f294d5c24e8bbee2.arrow\n",
      "Loading cached processed dataset at C:\\Users\\KevinChou\\.cache\\huggingface\\datasets\\json\\default-0cae1a5ed046e7ee\\0.0.0\\e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4\\cache-40c0019c2c1d69f4.arrow\n",
      "Loading cached processed dataset at C:\\Users\\KevinChou\\.cache\\huggingface\\datasets\\json\\default-891c751e5dc163e9\\0.0.0\\e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4\\cache-a3e539b82b39e4b2.arrow\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import datasets \n",
    "from datasets import load_dataset\n",
    "from accelerate import Accelerator\n",
    "import torch\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "import transformers\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    CONFIG_MAPPING,\n",
    "    MODEL_MAPPING,\n",
    "    AdamW,\n",
    "    AutoConfig,\n",
    "    AutoModelForSeq2SeqLM,\n",
    "    DataCollatorForSeq2Seq,\n",
    "    default_data_collator,\n",
    "    get_scheduler,\n",
    "    set_seed,\n",
    ")\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "dataset = load_dataset(\"json\", data_files=\"train.json\")\n",
    "dataset_test = load_dataset(\"json\", data_files=\"test.json\")\n",
    "dataset_eval = load_dataset(\"json\", data_files=\"eval.json\")\n",
    "dataset['eval'] = dataset_eval['train']\n",
    "dataset['test'] = dataset_test['train']\n",
    "dataset\n",
    "mt5_tokenizer = AutoTokenizer.from_pretrained(\"google/mt5-small\")\n",
    "\n",
    "def tokenize_sample_data(data):\n",
    "  # Max token size is 14536 and 215 for inputs and labels, respectively.\n",
    "  # Here I restrict these token size.\n",
    "  input_feature = mt5_tokenizer(data[\"article\"], truncation=True, max_length=256)\n",
    "  label = mt5_tokenizer(data[\"title\"], truncation=True, max_length=64)\n",
    "  return {\n",
    "    \"input_ids\": input_feature[\"input_ids\"],\n",
    "    \"attention_mask\": input_feature[\"attention_mask\"],\n",
    "    \"labels\": label[\"input_ids\"],\n",
    "  }\n",
    "\n",
    "tokenized_ds = dataset.map(\n",
    "  tokenize_sample_data,\n",
    "  remove_columns=[\"id\", \"title\", \"url\", \"article\"],\n",
    "  batched=True,\n",
    "  batch_size=128)\n",
    "\n",
    "\n",
    "\n",
    "import evaluate\n",
    "import numpy as np\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "\n",
    "rouge_metric = evaluate.load(\"rouge\")\n",
    "\n",
    "# define function for custom tokenization\n",
    "def tokenize_sentence(arg):\n",
    "  encoded_arg = mt5_tokenizer(arg)\n",
    "  return mt5_tokenizer.convert_ids_to_tokens(encoded_arg.input_ids)\n",
    "# define function to get ROUGE scores with custom tokenization\n",
    "def metrics_func(eval_arg):\n",
    "  preds, labels = eval_arg\n",
    "  labels = np.where(labels != -100, labels, mt5_tokenizer.pad_token_id)\n",
    "  text_preds = mt5_tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
    "  text_labels = mt5_tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "  text_preds = [(p if p.endswith((\"!\", \"！\", \"?\", \"？\", \"。\")) else p + \"。\") for p in text_preds]\n",
    "  text_labels = [(l if l.endswith((\"!\", \"！\", \"?\", \"？\", \"。\")) else l + \"。\") for l in text_labels]\n",
    "  sent_tokenizer_tw = RegexpTokenizer(u'[^!！?？。]*[!！?？。]')\n",
    "  text_preds = [\"\\n\".join(np.char.strip(sent_tokenizer_tw.tokenize(p))) for p in text_preds]\n",
    "  text_labels = [\"\\n\".join(np.char.strip(sent_tokenizer_tw.tokenize(l))) for l in text_labels]\n",
    "  # compute ROUGE score with custom tokenization\n",
    "\n",
    "  return rouge_metric.compute(\n",
    "    predictions=text_preds,\n",
    "    references=text_labels,\n",
    "    tokenizer=tokenize_sentence\n",
    "  )\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mt5_config = AutoConfig.from_pretrained(\n",
    "  \"google/mt5-small\",\n",
    "  max_length=128,\n",
    "  length_penalty=0.6,\n",
    "  no_repeat_ngram_size=2,\n",
    "  num_beams=10,\n",
    ")\n",
    "model = (AutoModelForSeq2SeqLM\n",
    "         .from_pretrained(\"google/mt5-small\", config=mt5_config)\n",
    "         .to(device))\n",
    "\n",
    "data_collator = DataCollatorForSeq2Seq(\n",
    "  mt5_tokenizer,\n",
    "  model=model,\n",
    "  return_tensors=\"pt\")\n",
    "\n",
    "sample_dataloader = DataLoader(\n",
    "  tokenized_ds[\"test\"].with_format(\"torch\"),\n",
    "  collate_fn=data_collator,\n",
    "  batch_size=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "756\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 756/756 [22:10<00:00,  1.76s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'rouge1': 0.1406464367690917,\n",
       " 'rouge2': 0.06483060634646234,\n",
       " 'rougeL': 0.1382132647823016,\n",
       " 'rougeLsum': 0.13735610576505136}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "print(len(sample_dataloader))\n",
    "\n",
    "count = 0\n",
    "total_metrice = {\n",
    "  'rouge1': 0,\n",
    "  'rouge2': 0,\n",
    "  'rougeL': 0,\n",
    "  'rougeLsum': 0\n",
    "}\n",
    "for batch in tqdm(sample_dataloader):\n",
    "  #print(batch)\n",
    "  count += 1\n",
    "  with torch.no_grad():\n",
    "    preds = model.generate(\n",
    "      batch[\"input_ids\"].to(device),\n",
    "      num_beams=10,\n",
    "      num_return_sequences=1,\n",
    "      no_repeat_ngram_size=1,\n",
    "      remove_invalid_values=True,\n",
    "      max_length=64,\n",
    "    )\n",
    "  labels = batch[\"labels\"]\n",
    "  met = metrics_func([preds, labels])\n",
    "  total_metrice['rouge1'] += met['rouge1']\n",
    "  total_metrice['rouge2'] += met['rouge2']\n",
    "  total_metrice['rougeL'] += met['rougeL']\n",
    "  total_metrice['rougeLsum'] += met['rougeLsum']\n",
    "total_metrice['rouge1'] /= count\n",
    "total_metrice['rouge2'] /= count\n",
    "total_metrice['rougeL'] /= count\n",
    "total_metrice['rougeLsum'] /= count\n",
    "total_metrice\n",
    "#metrics_func([preds, labels])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "mt5_config = AutoConfig.from_pretrained(\n",
    "  \"google/mt5-small\",\n",
    "  max_length=128,\n",
    "  length_penalty=0.6,\n",
    "  no_repeat_ngram_size=2,\n",
    "  num_beams=10,\n",
    ")\n",
    "model = (AutoModelForSeq2SeqLM\n",
    "         .from_pretrained(\"./iter_trained_for_summarization_tw\", config=mt5_config)\n",
    "         .to(device))\n",
    "\n",
    "data_collator = DataCollatorForSeq2Seq(\n",
    "  mt5_tokenizer,\n",
    "  model=model,\n",
    "  return_tensors=\"pt\")\n",
    "\n",
    "sample_dataloader = DataLoader(\n",
    "  tokenized_ds[\"test\"].with_format(\"torch\"),\n",
    "  collate_fn=data_collator,\n",
    "  batch_size=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "756\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 756/756 [27:09<00:00,  2.15s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'rouge1': 0.44033605066969383,\n",
       " 'rouge2': 0.22614575892683023,\n",
       " 'rougeL': 0.3753717622318992,\n",
       " 'rougeLsum': 0.4061266685861491}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "print(len(sample_dataloader))\n",
    "count = 0\n",
    "total_metrice = {\n",
    "  'rouge1': 0,\n",
    "  'rouge2': 0,\n",
    "  'rougeL': 0,\n",
    "  'rougeLsum': 0\n",
    "}\n",
    "for batch in tqdm(sample_dataloader):\n",
    "  #print(batch)\n",
    "  count += 1\n",
    "  with torch.no_grad():\n",
    "    preds = model.generate(\n",
    "      batch[\"input_ids\"].to(device),\n",
    "      num_beams=10,\n",
    "      num_return_sequences=1,\n",
    "      no_repeat_ngram_size=1,\n",
    "      remove_invalid_values=True,\n",
    "      max_length=64,\n",
    "    )\n",
    "  labels = batch[\"labels\"]\n",
    "  met = metrics_func([preds, labels])\n",
    "  total_metrice['rouge1'] += met['rouge1']\n",
    "  total_metrice['rouge2'] += met['rouge2']\n",
    "  total_metrice['rougeL'] += met['rougeL']\n",
    "  total_metrice['rougeLsum'] += met['rougeLsum']\n",
    "total_metrice['rouge1'] /= count\n",
    "total_metrice['rouge2'] /= count\n",
    "total_metrice['rougeL'] /= count\n",
    "total_metrice['rougeLsum'] /= count\n",
    "total_metrice\n",
    "#metrics_func([preds, labels])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\AI_project\\AI_env\\lib\\site-packages\\transformers\\convert_slow_tokenizer.py:454: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from transformers import AutoTokenizer\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "import evaluate\n",
    "\n",
    "mt5_tokenizer = AutoTokenizer.from_pretrained(\"google/mt5-small\")\n",
    "\n",
    "rouge_metric = evaluate.load(\"rouge\")\n",
    "\n",
    "\n",
    "def tokenize_sentence(arg):\n",
    "  encoded_arg = mt5_tokenizer(arg)\n",
    "  return mt5_tokenizer.convert_ids_to_tokens(encoded_arg.input_ids)\n",
    "\n",
    "def eval_func(eval_arg):\n",
    "    text_preds, text_labels = eval_arg\n",
    "    text_preds = [(p if p.endswith((\"!\", \"！\", \"?\", \"？\", \"。\")) else p + \"。\") for p in text_preds]\n",
    "    text_labels = [(l if l.endswith((\"!\", \"！\", \"?\", \"？\", \"。\")) else l + \"。\") for l in text_labels]\n",
    "    sent_tokenizer_tw = RegexpTokenizer(u'[^!！?？。]*[!！?？。]')\n",
    "    text_preds = [\"\\n\".join(np.char.strip(sent_tokenizer_tw.tokenize(p))) for p in text_preds]\n",
    "    text_labels = [\"\\n\".join(np.char.strip(sent_tokenizer_tw.tokenize(l))) for l in text_labels]\n",
    "    return rouge_metric.compute(\n",
    "        predictions=text_preds,\n",
    "        references=text_labels,\n",
    "        tokenizer=tokenize_sentence\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3777/3777 [00:00<00:00, 236696.89it/s]\n",
      "302it [00:30,  9.83it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 24\u001b[0m\n\u001b[0;32m     22\u001b[0m label \u001b[39m=\u001b[39m [label]\n\u001b[0;32m     23\u001b[0m prebs \u001b[39m=\u001b[39m [prebs]\n\u001b[1;32m---> 24\u001b[0m met \u001b[39m=\u001b[39m eval_func([prebs, label])\n\u001b[0;32m     25\u001b[0m count \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m     26\u001b[0m total_metrice_extract[\u001b[39m'\u001b[39m\u001b[39mrouge1\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m met[\u001b[39m'\u001b[39m\u001b[39mrouge1\u001b[39m\u001b[39m'\u001b[39m]\n",
      "Cell \u001b[1;32mIn[1], line 22\u001b[0m, in \u001b[0;36meval_func\u001b[1;34m(eval_arg)\u001b[0m\n\u001b[0;32m     20\u001b[0m text_preds \u001b[39m=\u001b[39m [\u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mjoin(np\u001b[39m.\u001b[39mchar\u001b[39m.\u001b[39mstrip(sent_tokenizer_tw\u001b[39m.\u001b[39mtokenize(p))) \u001b[39mfor\u001b[39;00m p \u001b[39min\u001b[39;00m text_preds]\n\u001b[0;32m     21\u001b[0m text_labels \u001b[39m=\u001b[39m [\u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mjoin(np\u001b[39m.\u001b[39mchar\u001b[39m.\u001b[39mstrip(sent_tokenizer_tw\u001b[39m.\u001b[39mtokenize(l))) \u001b[39mfor\u001b[39;00m l \u001b[39min\u001b[39;00m text_labels]\n\u001b[1;32m---> 22\u001b[0m \u001b[39mreturn\u001b[39;00m rouge_metric\u001b[39m.\u001b[39;49mcompute(\n\u001b[0;32m     23\u001b[0m     predictions\u001b[39m=\u001b[39;49mtext_preds,\n\u001b[0;32m     24\u001b[0m     references\u001b[39m=\u001b[39;49mtext_labels,\n\u001b[0;32m     25\u001b[0m     tokenizer\u001b[39m=\u001b[39;49mtokenize_sentence\n\u001b[0;32m     26\u001b[0m )\n",
      "File \u001b[1;32md:\\AI_project\\AI_env\\lib\\site-packages\\evaluate\\module.py:444\u001b[0m, in \u001b[0;36mEvaluationModule.compute\u001b[1;34m(self, predictions, references, **kwargs)\u001b[0m\n\u001b[0;32m    442\u001b[0m inputs \u001b[39m=\u001b[39m {input_name: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdata[input_name] \u001b[39mfor\u001b[39;00m input_name \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_feature_names()}\n\u001b[0;32m    443\u001b[0m \u001b[39mwith\u001b[39;00m temp_seed(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mseed):\n\u001b[1;32m--> 444\u001b[0m     output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compute(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39minputs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mcompute_kwargs)\n\u001b[0;32m    446\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbuf_writer \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    447\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbuf_writer \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\.cache\\huggingface\\modules\\evaluate_modules\\metrics\\evaluate-metric--rouge\\b01e0accf3bd6dd24839b769a5fda24e14995071570870922c71970b3a6ed886\\rouge.py:149\u001b[0m, in \u001b[0;36mRouge._compute\u001b[1;34m(self, predictions, references, rouge_types, use_aggregator, use_stemmer, tokenizer)\u001b[0m\n\u001b[0;32m    146\u001b[0m         scores\u001b[39m.\u001b[39mappend(score)\n\u001b[0;32m    148\u001b[0m \u001b[39mif\u001b[39;00m use_aggregator:\n\u001b[1;32m--> 149\u001b[0m     result \u001b[39m=\u001b[39m aggregator\u001b[39m.\u001b[39;49maggregate()\n\u001b[0;32m    150\u001b[0m     \u001b[39mfor\u001b[39;00m key \u001b[39min\u001b[39;00m result:\n\u001b[0;32m    151\u001b[0m         result[key] \u001b[39m=\u001b[39m result[key]\u001b[39m.\u001b[39mmid\u001b[39m.\u001b[39mfmeasure\n",
      "File \u001b[1;32md:\\AI_project\\AI_env\\lib\\site-packages\\rouge_score\\scoring.py:124\u001b[0m, in \u001b[0;36mBootstrapAggregator.aggregate\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    122\u001b[0m score_matrix \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mvstack(\u001b[39mtuple\u001b[39m(scores))\n\u001b[0;32m    123\u001b[0m \u001b[39m# Percentiles are returned as (interval, measure).\u001b[39;00m\n\u001b[1;32m--> 124\u001b[0m percentiles \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_bootstrap_resample(score_matrix)\n\u001b[0;32m    125\u001b[0m \u001b[39m# Extract the three intervals (low, mid, high).\u001b[39;00m\n\u001b[0;32m    126\u001b[0m intervals \u001b[39m=\u001b[39m \u001b[39mtuple\u001b[39m(\n\u001b[0;32m    127\u001b[0m     (scores[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m(\u001b[39m*\u001b[39mpercentiles[j, :]) \u001b[39mfor\u001b[39;00m j \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m3\u001b[39m)))\n",
      "File \u001b[1;32md:\\AI_project\\AI_env\\lib\\site-packages\\rouge_score\\scoring.py:149\u001b[0m, in \u001b[0;36mBootstrapAggregator._bootstrap_resample\u001b[1;34m(self, matrix)\u001b[0m\n\u001b[0;32m    147\u001b[0m sample_mean \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mzeros((\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_n_samples, matrix\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m]))\n\u001b[0;32m    148\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_n_samples):\n\u001b[1;32m--> 149\u001b[0m   sample_idx \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39;49mrandom\u001b[39m.\u001b[39;49mchoice(\n\u001b[0;32m    150\u001b[0m       np\u001b[39m.\u001b[39;49marange(matrix\u001b[39m.\u001b[39;49mshape[\u001b[39m0\u001b[39;49m]), size\u001b[39m=\u001b[39;49mmatrix\u001b[39m.\u001b[39;49mshape[\u001b[39m0\u001b[39;49m])\n\u001b[0;32m    151\u001b[0m   sample \u001b[39m=\u001b[39m matrix[sample_idx, :]\n\u001b[0;32m    152\u001b[0m   sample_mean[i, :] \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mmean(sample, axis\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#import eval\n",
    "import json \n",
    "from tqdm import tqdm\n",
    "topic_labels = []\n",
    "topic_prebs = []\n",
    "with open(\"extract.json\", encoding='utf-8') as file:\n",
    "    datas = list(file)\n",
    "total_metrice_extract = {\n",
    "  'rouge1': 0,\n",
    "  'rouge2': 0,\n",
    "  'rougeL': 0,\n",
    "  'rougeLsum': 0\n",
    "}\n",
    "for data in tqdm(datas):\n",
    "    data = json.loads(data)\n",
    "    topic_labels.append(data[\"old_title\"])\n",
    "    topic_prebs.append(data[\"new_title\"])\n",
    "#topic_labels\n",
    "#topic_prebs\n",
    "count = 0\n",
    "for label, prebs in tqdm(zip(topic_labels, topic_prebs)):\n",
    "    label = [label]\n",
    "    prebs = [prebs]\n",
    "    met = eval_func([prebs, label])\n",
    "    count += 1\n",
    "    total_metrice_extract['rouge1'] += met['rouge1']\n",
    "    total_metrice_extract['rouge2'] += met['rouge2']\n",
    "    total_metrice_extract['rougeL'] += met['rougeL']\n",
    "    total_metrice_extract['rougeLsum'] += met['rougeLsum']\n",
    "    #print(score)\n",
    "total_metrice_extract['rouge1'] /= count\n",
    "total_metrice_extract['rouge2'] /= count\n",
    "total_metrice_extract['rougeL'] /= count\n",
    "total_metrice_extract['rougeLsum'] /= count\n",
    "total_metrice_extract"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AI_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
