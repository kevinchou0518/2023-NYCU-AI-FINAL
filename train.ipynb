{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import datasets \n",
    "from datasets import load_dataset\n",
    "from accelerate import Accelerator\n",
    "import torch\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "import transformers\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    CONFIG_MAPPING,\n",
    "    MODEL_MAPPING,\n",
    "    AdamW,\n",
    "    AutoConfig,\n",
    "    AutoModelForSeq2SeqLM,\n",
    "    DataCollatorForSeq2Seq,\n",
    "    default_data_collator,\n",
    "    get_scheduler,\n",
    "    set_seed,\n",
    ")\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset json (C:/Users/KevinChou/.cache/huggingface/datasets/json/default-6a36e5922eb05f45/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "42bdfefdb98d49e19a69c1be98806b8a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset json (C:/Users/KevinChou/.cache/huggingface/datasets/json/default-891c751e5dc163e9/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b80622a82b34973a6faafc537dcaaa8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset json (C:/Users/KevinChou/.cache/huggingface/datasets/json/default-0cae1a5ed046e7ee/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "506b225dabdf4c2d8e764f479546a80d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\AI_project\\AI_env\\lib\\site-packages\\transformers\\convert_slow_tokenizer.py:454: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "dataset = load_dataset(\"json\", data_files=\"train.json\")\n",
    "dataset_test = load_dataset(\"json\", data_files=\"test.json\")\n",
    "dataset_eval = load_dataset(\"json\", data_files=\"eval.json\")\n",
    "dataset['eval'] = dataset_eval['train']\n",
    "dataset['test'] = dataset_test['train']\n",
    "dataset\n",
    "mt5_tokenizer = AutoTokenizer.from_pretrained(\"google/mt5-small\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at C:\\Users\\KevinChou\\.cache\\huggingface\\datasets\\json\\default-6a36e5922eb05f45\\0.0.0\\e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4\\cache-f294d5c24e8bbee2.arrow\n",
      "Loading cached processed dataset at C:\\Users\\KevinChou\\.cache\\huggingface\\datasets\\json\\default-0cae1a5ed046e7ee\\0.0.0\\e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4\\cache-40c0019c2c1d69f4.arrow\n",
      "Loading cached processed dataset at C:\\Users\\KevinChou\\.cache\\huggingface\\datasets\\json\\default-891c751e5dc163e9\\0.0.0\\e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4\\cache-a3e539b82b39e4b2.arrow\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 30221\n",
       "    })\n",
       "    eval: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 3777\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 3777\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def tokenize_sample_data(data):\n",
    "  # Max token size is 14536 and 215 for inputs and labels, respectively.\n",
    "  # Here I restrict these token size.\n",
    "  input_feature = mt5_tokenizer(data[\"article\"], truncation=True, max_length=256)\n",
    "  label = mt5_tokenizer(data[\"title\"], truncation=True, max_length=64)\n",
    "  return {\n",
    "    \"input_ids\": input_feature[\"input_ids\"],\n",
    "    \"attention_mask\": input_feature[\"attention_mask\"],\n",
    "    \"labels\": label[\"input_ids\"],\n",
    "  }\n",
    "\n",
    "tokenized_ds = dataset.map(\n",
    "  tokenize_sample_data,\n",
    "  remove_columns=[\"id\", \"title\", \"url\", \"article\"],\n",
    "  batched=True,\n",
    "  batch_size=128)\n",
    "\n",
    "tokenized_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# see https://huggingface.co/docs/transformers/main_classes/configuration\n",
    "mt5_config = AutoConfig.from_pretrained(\n",
    "  \"google/mt5-small\",\n",
    "  max_length=128,\n",
    "  length_penalty=0.6,\n",
    "  no_repeat_ngram_size=2,\n",
    "  num_beams=15,\n",
    ")\n",
    "model = (AutoModelForSeq2SeqLM\n",
    "         .from_pretrained(\"google/mt5-small\", config=mt5_config)\n",
    "         .to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data_collator = DataCollatorForSeq2Seq(\n",
    "  mt5_tokenizer,\n",
    "  model=model,\n",
    "  return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "import numpy as np\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from utils import twrouge \n",
    "\n",
    "rouge_metric = evaluate.load(\"rouge\")\n",
    "\n",
    "# define function for custom tokenization\n",
    "def tokenize_sentence(arg):\n",
    "  encoded_arg = mt5_tokenizer(arg)\n",
    "  return mt5_tokenizer.convert_ids_to_tokens(encoded_arg.input_ids)\n",
    "# define function to get ROUGE scores with custom tokenization\n",
    "def metrics_func(eval_arg):\n",
    "  preds, labels = eval_arg\n",
    "  labels = np.where(labels != -100, labels, mt5_tokenizer.pad_token_id)\n",
    "  text_preds = mt5_tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
    "  text_labels = mt5_tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "  text_preds = [(p if p.endswith((\"!\", \"！\", \"?\", \"？\", \"。\")) else p + \"。\") for p in text_preds]\n",
    "  text_labels = [(l if l.endswith((\"!\", \"！\", \"?\", \"？\", \"。\")) else l + \"。\") for l in text_labels]\n",
    "  sent_tokenizer_tw = RegexpTokenizer(u'[^!！?？。]*[!！?？。]')\n",
    "  text_preds = [\"\\n\".join(np.char.strip(sent_tokenizer_tw.tokenize(p))) for p in text_preds]\n",
    "  text_labels = [\"\\n\".join(np.char.strip(sent_tokenizer_tw.tokenize(l))) for l in text_labels]\n",
    "  # compute ROUGE score with custom tokenization\n",
    "\n",
    "  return rouge_metric.compute(\n",
    "    predictions=text_preds,\n",
    "    references=text_labels,\n",
    "    tokenizer=tokenize_sentence\n",
    "  )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[   259, 210694,   5072,  ..., 147891,    292,      1],\n",
      "        [   259,  22746,   9911,  ...,  44126,    261,      1],\n",
      "        [   259,  48734, 191679,  ..., 111357,  57956,      1],\n",
      "        [   259, 210694,   5072,  ...,   3541, 136360,      1],\n",
      "        [ 78022,    276,    259,  ...,    261,   3236,      1]]), 'attention_mask': tensor([[1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 1, 1, 1]]), 'labels': tensor([[   259,  73849,  26145,   2144,  36470,   3862,  99722, 102471, 102031,\n",
      "            309,   6874, 201640,   1193, 127986,    879,   3916,  90821, 112530,\n",
      "          21014,    267,   1637, 176430,  47728,  27333,    939,   4833,  23281,\n",
      "           8882, 153832,  10559,   4153, 223367,    879,      1],\n",
      "        [   259, 134176,  15778,  23175, 232014,    410,   1146,   5742,  33692,\n",
      "          31225, 210707,    259, 183548,    292,  25231,  69196,    292,   3586,\n",
      "           1193, 237359,   4110,  58115,   5160,      1,   -100,   -100,   -100,\n",
      "           -100,   -100,   -100,   -100,   -100,   -100,   -100],\n",
      "        [   259,  66760,   2445,   1453,  66241,    292,   5853, 151146,  10722,\n",
      "           7613,    259,  32627, 111816,   2334,    788,  26112, 103695,   7613,\n",
      "         148405,  76972,  66760,      1,   -100,   -100,   -100,   -100,   -100,\n",
      "           -100,   -100,   -100,   -100,   -100,   -100,   -100],\n",
      "        [   259,  31377,  11201,  61343, 152645,  24281, 130160,  92424,  18850,\n",
      "          21270, 249357,    259, 162115, 201904, 228226, 148405,   2811, 204062,\n",
      "           1087,  33692,      1,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "           -100,   -100,   -100,   -100,   -100,   -100,   -100],\n",
      "        [   259,   7431,  19144,  30688,   5507,  26643,    309,  27129,  21618,\n",
      "          23637,  96105, 184275,   4805,  23094,   9505,    475,    259,  11848,\n",
      "           3094,  70845,   1193, 108789,  59089,      1,   -100,   -100,   -100,\n",
      "           -100,   -100,   -100,   -100,   -100,   -100,   -100]]), 'decoder_input_ids': tensor([[     0,    259,  73849,  26145,   2144,  36470,   3862,  99722, 102471,\n",
      "         102031,    309,   6874, 201640,   1193, 127986,    879,   3916,  90821,\n",
      "         112530,  21014,    267,   1637, 176430,  47728,  27333,    939,   4833,\n",
      "          23281,   8882, 153832,  10559,   4153, 223367,    879],\n",
      "        [     0,    259, 134176,  15778,  23175, 232014,    410,   1146,   5742,\n",
      "          33692,  31225, 210707,    259, 183548,    292,  25231,  69196,    292,\n",
      "           3586,   1193, 237359,   4110,  58115,   5160,      1,      0,      0,\n",
      "              0,      0,      0,      0,      0,      0,      0],\n",
      "        [     0,    259,  66760,   2445,   1453,  66241,    292,   5853, 151146,\n",
      "          10722,   7613,    259,  32627, 111816,   2334,    788,  26112, 103695,\n",
      "           7613, 148405,  76972,  66760,      1,      0,      0,      0,      0,\n",
      "              0,      0,      0,      0,      0,      0,      0],\n",
      "        [     0,    259,  31377,  11201,  61343, 152645,  24281, 130160,  92424,\n",
      "          18850,  21270, 249357,    259, 162115, 201904, 228226, 148405,   2811,\n",
      "         204062,   1087,  33692,      1,      0,      0,      0,      0,      0,\n",
      "              0,      0,      0,      0,      0,      0,      0],\n",
      "        [     0,    259,   7431,  19144,  30688,   5507,  26643,    309,  27129,\n",
      "          21618,  23637,  96105, 184275,   4805,  23094,   9505,    475,    259,\n",
      "          11848,   3094,  70845,   1193, 108789,  59089,      1,      0,      0,\n",
      "              0,      0,      0,      0,      0,      0,      0]])}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'rouge1': 0.13491165114694526,\n",
       " 'rouge2': 0.06066053511705686,\n",
       " 'rougeL': 0.1362016806722689,\n",
       " 'rougeLsum': 0.13500449415759513}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "sample_dataloader = DataLoader(\n",
    "  tokenized_ds[\"test\"].with_format(\"torch\"),\n",
    "  collate_fn=data_collator,\n",
    "  batch_size=5)\n",
    "for batch in sample_dataloader:\n",
    "  print(batch)\n",
    "  with torch.no_grad():\n",
    "    preds = model.generate(\n",
    "      batch[\"input_ids\"].to(device),\n",
    "      num_beams=15,\n",
    "      num_return_sequences=1,\n",
    "      no_repeat_ngram_size=1,\n",
    "      remove_invalid_values=True,\n",
    "      max_length=128,\n",
    "    )\n",
    "  labels = batch[\"labels\"]\n",
    "  break\n",
    "\n",
    "metrics_func([preds, labels])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Seq2SeqTrainingArguments\n",
    "\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "  output_dir = \"mt5-summarize-tw\",\n",
    "  log_level = \"error\",\n",
    "  num_train_epochs = 10,\n",
    "  learning_rate = 5e-4,\n",
    "  lr_scheduler_type = \"linear\",\n",
    "  warmup_steps = 90,\n",
    "  optim = \"adafactor\",\n",
    "  weight_decay = 0.01,\n",
    "  per_device_train_batch_size = 2,\n",
    "  per_device_eval_batch_size = 1,\n",
    "  gradient_accumulation_steps = 16,\n",
    "  evaluation_strategy = \"steps\",\n",
    "  eval_steps = 100,\n",
    "  predict_with_generate=True,\n",
    "  generation_max_length = 128,\n",
    "  save_steps = 500,\n",
    "  logging_steps = 10,\n",
    "  push_to_hub = False\n",
    ")\n",
    "tokenized_ds[\"train\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "traindataset = tokenized_ds[\"train\"]\n",
    "evaldataset = tokenized_ds[\"eval\"]\n",
    "train_dataloader = DataLoader(\n",
    "    traindataset, shuffle=True, collate_fn=data_collator, batch_size=2\n",
    ")\n",
    "eval_dataloader = DataLoader(\n",
    "    evaldataset, shuffle=True, collate_fn=data_collator, batch_size=2\n",
    ")\n",
    "no_decay = [\"bias\", \"LayerNorm.weight\"]\n",
    "optimizer_grouped_parameters = [\n",
    "        {\n",
    "            \"params\": [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)],\n",
    "            \"weight_decay\": 0.01,\n",
    "        },\n",
    "        {\n",
    "            \"params\": [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)],\n",
    "            \"weight_decay\": 0.0,\n",
    "        },\n",
    "    ]\n",
    "optimizer = AdamW(optimizer_grouped_parameters, lr=5e-4)\n",
    "accelerator = Accelerator()\n",
    "model, optimizer, train_dataloader, eval_dataloader = accelerator.prepare(\n",
    "        model, optimizer, train_dataloader, eval_dataloader\n",
    ")\n",
    "lr_scheduler = get_scheduler(\n",
    "        name='linear',\n",
    "        optimizer=optimizer,\n",
    "        num_warmup_steps=90,\n",
    "        num_training_steps=4720,\n",
    ")\n",
    "for epoch in range(4):\n",
    "    model.train()\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "        output = model(**batch)\n",
    "        loss = output.loss\n",
    "        loss = loss / 16\n",
    "        print(loss)\n",
    "        accelerator.backward(loss)\n",
    "        if step % 16 == 0 or step == len(train_dataloader) - 1:\n",
    "            optimizer.step()\n",
    "            lr_scheduler.step()\n",
    "            optimizer.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Seq2SeqTrainer\n",
    "trainer = Seq2SeqTrainer(\n",
    "  model = model,\n",
    "  args = training_args,\n",
    "  data_collator = data_collator,\n",
    "  compute_metrics = metrics_func,\n",
    "  train_dataset = tokenized_ds[\"train\"],\n",
    "  eval_dataset = tokenized_ds[\"eval\"].select(range(20)),\n",
    "  tokenizer = mt5_tokenizer,\n",
    ")\n",
    "\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from transformers import AutoModelForSeq2SeqLM\n",
    "\n",
    "# save fine-tuned model in local\n",
    "os.makedirs(\"./trained_for_summarization_tw\", exist_ok=True)\n",
    "if hasattr(trainer.model, \"module\"):\n",
    "  trainer.model.module.save_pretrained(\"./trained_for_summarization_tw\")\n",
    "else:\n",
    "  trainer.model.save_pretrained(\"./trained_for_summarization_tw\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'rouge1': 0.44615599088573976, 'rouge2': 0.2272960247928959, 'rougeL': 0.3639858793497608, 'rougeLsum': 0.39796679438058746}\n",
      "***** Input's Text *****\n",
      "Molly　圖若是問到男人心中的理想女友輪廓，應該很多女生的第一反應都會說出甜美臉蛋、好身材、長髮或是氣質好等這些「大眾印象」中的條件，但實際上卻跟我們想像的差很大！國外男性網站《AskMen》曾票選出「理想女友」的10個特質，不僅外在條件佔比不高，第一名更讓人完全沒想到。雖然男生都喜歡被需要、被依靠的感覺，但若是沒有男友就什麼都不行，無時無刻都要緊黏著另一半，這樣「過度依賴」，只會讓人覺得厭煩。男生喜歡的是平時很獨立，但有時又會柔弱、需要他們保護的女生。男生喜歡聰明的女生並不是指要EQ 180、上知天文下知地理，而是能言之有物，擁有有趣的內在。如果空有美麗外表，內在空洞，聊天都聊不下去了，又如何愛上你。兩人在一起，除了個性、三觀是否契合外，性生活也是很重要的一環，和諧的性生活能讓雙方感情更加親密，但也不是指女生要一昧配合男生，而是能找雙方都能享受、喜歡的方式才對。有吸引力並不只侷限於漂亮的臉蛋、性感的身材，像是有人是腿控，腿美就加分；有人喜愛開朗的笑容，只要面對甜甜微笑就被融化，對男生而言，外表只是第一眼，魅力才是吸引他們的重點。人和人之間的相處，最基本的就是尊重，就算再親密也不能忽略。大部分男生都很討厭被管束的感覺，尤其兩人在一起最重要的就是能互相尊重，應該要尊重對方有自己的時間、意志和選擇，不要以「愛」之名，做出各種限制、干涉，給予適當的自由空間，兩人感情也能更長久。還記得學生時期，我們都很討厭被爸媽碎碎念嗎？當然情侶之間也一樣，如果無法理性溝通，只是一直嘮叨碎念，不僅會讓他失去耐心，妳也會變成他另一個「媽媽」。不論男女，每個人都會希望伴侶跟自己的家人、好友群們能相處融洽，彼此可以自在地融入對方的生活圈，畢竟人活在世上無法只顧愛情，誰都不太可能拋棄家人和朋友，只和另一半生活。沒有人是完美的，再優秀的人都會其不足之處，你無法只挑對方的優點愛而不接受他的缺點或壞習慣，要懂得包容、尊重對方，雙方一起找到平衡點，畢竟也不是只有你忍受他的壞習慣，同時他也在忍受你的。兩個人在一起，不是只有甜蜜的浪漫情懷，生活中是否能互相扶持、成為彼此最堅強的力量，也是一段成熟感情最吸引人的地方。尤其遇到一個好的另一半，會讓他們自然就想「be a better man」，給你更好的生活和未來。►►看更多時尚資訊分享，快來下載星光雲App \n",
      "***** Summary Text (True Value) *****\n",
      "身材不是最重要!男性票選理想女友特質Top 10 第一名讓人猜不到\n",
      "***** Summary Text (Generated Text) *****\n",
      "男人心中「理想女友」10個特質!內在條件佔比不高卻跟想像差很大\n",
      "5\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# load local model\n",
    "model = (AutoModelForSeq2SeqLM\n",
    "         .from_pretrained(\"./trained_for_summarization_tw\")\n",
    "         .to(device))\n",
    "\n",
    "# Predict with test data (first 5 rows)\n",
    "sample_dataloader = DataLoader(\n",
    "  tokenized_ds[\"test\"].with_format(\"torch\"),\n",
    "  collate_fn=data_collator,\n",
    "  batch_size=5)\n",
    "for batch in sample_dataloader:\n",
    "  with torch.no_grad():\n",
    "    preds = model.generate(\n",
    "      batch[\"input_ids\"].to(device),\n",
    "      num_beams=15,\n",
    "      num_return_sequences=1,\n",
    "      no_repeat_ngram_size=1,\n",
    "      remove_invalid_values=True,\n",
    "      max_length=128,\n",
    "    )\n",
    "  labels = batch[\"labels\"]\n",
    "  break\n",
    "def eval(eval_arg):\n",
    "    text_preds, text_labels = eval_arg\n",
    "    text_preds = [(p if p.endswith((\"!\", \"！\", \"?\", \"？\", \"。\")) else p + \"。\") for p in text_preds]\n",
    "    text_labels = [(l if l.endswith((\"!\", \"！\", \"?\", \"？\", \"。\")) else l + \"。\") for l in text_labels]\n",
    "    sent_tokenizer_tw = RegexpTokenizer(u'[^!！?？。]*[!！?？。]')\n",
    "    text_preds = [\"\\n\".join(np.char.strip(sent_tokenizer_tw.tokenize(p))) for p in text_preds]\n",
    "    text_labels = [\"\\n\".join(np.char.strip(sent_tokenizer_tw.tokenize(l))) for l in text_labels]\n",
    "    return rouge_metric.compute(\n",
    "        predictions=text_preds,\n",
    "        references=text_labels,\n",
    "        tokenizer=tokenize_sentence\n",
    "    )\n",
    "# Replace -100 (see above)\n",
    "labels = np.where(labels != -100, labels, mt5_tokenizer.pad_token_id)\n",
    "\n",
    "# Convert id tokens to text\n",
    "text_preds = mt5_tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
    "text_labels = mt5_tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "print(eval([text_preds, text_labels]))\n",
    "# Show result\n",
    "print(\"***** Input's Text *****\")\n",
    "print(dataset[\"test\"][\"article\"][4])\n",
    "print(\"***** Summary Text (True Value) *****\")\n",
    "print(text_labels[4])\n",
    "print(\"***** Summary Text (Generated Text) *****\")\n",
    "print(text_preds[4])\n",
    "print(len(text_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[   259, 210694,   5072,  ..., 147891,    292,      1],\n",
      "        [   259,  22746,   9911,  ...,  44126,    261,      1],\n",
      "        [   259,  48734, 191679,  ..., 111357,  57956,      1],\n",
      "        [   259, 210694,   5072,  ...,   3541, 136360,      1],\n",
      "        [ 78022,    276,    259,  ...,    261,   3236,      1]]), 'attention_mask': tensor([[1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 1, 1, 1]]), 'labels': tensor([[   259,  73849,  26145,   2144,  36470,   3862,  99722, 102471, 102031,\n",
      "            309,   6874, 201640,   1193, 127986,    879,   3916,  90821, 112530,\n",
      "          21014,    267,   1637, 176430,  47728,  27333,    939,   4833,  23281,\n",
      "           8882, 153832,  10559,   4153, 223367,    879,      1],\n",
      "        [   259, 134176,  15778,  23175, 232014,    410,   1146,   5742,  33692,\n",
      "          31225, 210707,    259, 183548,    292,  25231,  69196,    292,   3586,\n",
      "           1193, 237359,   4110,  58115,   5160,      1,   -100,   -100,   -100,\n",
      "           -100,   -100,   -100,   -100,   -100,   -100,   -100],\n",
      "        [   259,  66760,   2445,   1453,  66241,    292,   5853, 151146,  10722,\n",
      "           7613,    259,  32627, 111816,   2334,    788,  26112, 103695,   7613,\n",
      "         148405,  76972,  66760,      1,   -100,   -100,   -100,   -100,   -100,\n",
      "           -100,   -100,   -100,   -100,   -100,   -100,   -100],\n",
      "        [   259,  31377,  11201,  61343, 152645,  24281, 130160,  92424,  18850,\n",
      "          21270, 249357,    259, 162115, 201904, 228226, 148405,   2811, 204062,\n",
      "           1087,  33692,      1,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "           -100,   -100,   -100,   -100,   -100,   -100,   -100],\n",
      "        [   259,   7431,  19144,  30688,   5507,  26643,    309,  27129,  21618,\n",
      "          23637,  96105, 184275,   4805,  23094,   9505,    475,    259,  11848,\n",
      "           3094,  70845,   1193, 108789,  59089,      1,   -100,   -100,   -100,\n",
      "           -100,   -100,   -100,   -100,   -100,   -100,   -100]]), 'decoder_input_ids': tensor([[     0,    259,  73849,  26145,   2144,  36470,   3862,  99722, 102471,\n",
      "         102031,    309,   6874, 201640,   1193, 127986,    879,   3916,  90821,\n",
      "         112530,  21014,    267,   1637, 176430,  47728,  27333,    939,   4833,\n",
      "          23281,   8882, 153832,  10559,   4153, 223367,    879],\n",
      "        [     0,    259, 134176,  15778,  23175, 232014,    410,   1146,   5742,\n",
      "          33692,  31225, 210707,    259, 183548,    292,  25231,  69196,    292,\n",
      "           3586,   1193, 237359,   4110,  58115,   5160,      1,      0,      0,\n",
      "              0,      0,      0,      0,      0,      0,      0],\n",
      "        [     0,    259,  66760,   2445,   1453,  66241,    292,   5853, 151146,\n",
      "          10722,   7613,    259,  32627, 111816,   2334,    788,  26112, 103695,\n",
      "           7613, 148405,  76972,  66760,      1,      0,      0,      0,      0,\n",
      "              0,      0,      0,      0,      0,      0,      0],\n",
      "        [     0,    259,  31377,  11201,  61343, 152645,  24281, 130160,  92424,\n",
      "          18850,  21270, 249357,    259, 162115, 201904, 228226, 148405,   2811,\n",
      "         204062,   1087,  33692,      1,      0,      0,      0,      0,      0,\n",
      "              0,      0,      0,      0,      0,      0,      0],\n",
      "        [     0,    259,   7431,  19144,  30688,   5507,  26643,    309,  27129,\n",
      "          21618,  23637,  96105, 184275,   4805,  23094,   9505,    475,    259,\n",
      "          11848,   3094,  70845,   1193, 108789,  59089,      1,      0,      0,\n",
      "              0,      0,      0,      0,      0,      0,      0]])}\n",
      "tensor([[     0,    259,  73849,  26145,   2144,  36470,  33692,   2371,    939,\n",
      "           5953,  11823,   3094,  65890,    879,    267,   4833,  23281,   8882,\n",
      "         153832,  10559,   4153, 223367,      1,      0,      0,      0,      0],\n",
      "        [     0,    259,   5761, 232014,   4583,   5070,    939,    838,  48913,\n",
      "           2372, 238922,    879,  53870,   3586,  23175,  54937,    309,   2445,\n",
      "         102619,   2550,    292,  47728, 143112,   4794,  10695,   1322,      1],\n",
      "        [     0,    259,  32627, 111816,   2334,    788,  26112,  76972,   5160,\n",
      "           2445,   1453,  66241,   4074,    309, 214960,   3377,   1193,   3801,\n",
      "          68221,   7321,  40594,  10722,   7613, 127099,      1,      0,      0],\n",
      "        [     0,    259, 241826, 141408,    292,  18850,  21270, 249357,  19438,\n",
      "          98623,    309, 162115, 201904, 228226,    939, 161160, 239369,   6892,\n",
      "          18904,  85899,    879,      1,      0,      0,      0,      0,      0],\n",
      "        [     0,    259,  32139, 129602,    939,  96105, 184275,    879,    660,\n",
      "          13605,   4805,  23094,    309,  51934,   1083, 148532, 239978,   7738,\n",
      "           1597,   2538, 171388,  18475,  76519,  15915, 144601,      1,      0]],\n",
      "       device='cuda:0')\n",
      "{'rouge1': 0.44615599088573976, 'rouge2': 0.2272960247928959, 'rougeL': 0.3639858793497608, 'rougeLsum': 0.39796679438058746}\n",
      "<class 'dict'>\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "sample_dataloader = DataLoader(\n",
    "  tokenized_ds[\"test\"].with_format(\"torch\"),\n",
    "  collate_fn=data_collator,\n",
    "  batch_size=5)\n",
    "for batch in sample_dataloader:\n",
    "  print(batch)\n",
    "  with torch.no_grad():\n",
    "    preds = model.generate(\n",
    "      batch[\"input_ids\"].to(device),\n",
    "      num_beams=15,\n",
    "      num_return_sequences=1,\n",
    "      no_repeat_ngram_size=1,\n",
    "      remove_invalid_values=True,\n",
    "      max_length=128,\n",
    "    )\n",
    "  labels = batch[\"labels\"]\n",
    "  break\n",
    "print(preds)\n",
    "print(metrics_func([preds, labels]))\n",
    "print(type(metrics_func([preds, labels])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AI_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
